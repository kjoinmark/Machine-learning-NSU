{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjoinmark/Machine-learning-NSU/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVIu6JsVpoHY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o1FnG27akbW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5A9NBRiasre"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuXoQ5Sg-PhY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\"\"\n",
        "Возьмите датасет Mnist (рукописные цифры от 0 до 9) и используйте каждый из\n",
        "известных вам классификаторов, сравните качество классификации, объясните\n",
        "почему одни из классификаторов работают лучше или хуже.\n",
        "\n",
        "KNN, decision tree, SVM, log reg\n",
        "\"\"\"\"\"\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "N = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Assd6Hjiot1h"
      },
      "outputs": [],
      "source": [
        "X_, X, Y_, Y = train_test_split(mnist.data, mnist.target, test_size=N, random_state=10)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz12INrlg0tm",
        "outputId": "3dad839c-94e7-43f7-e65d-68e50364bc9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "class\n",
              "0    1379\n",
              "1    1538\n",
              "2    1386\n",
              "3    1439\n",
              "4    1359\n",
              "5    1292\n",
              "6    1345\n",
              "7    1506\n",
              "8    1363\n",
              "9    1393\n",
              "Name: class, dtype: int64"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame({'class': Y_train })\n",
        "df.groupby(['class'])['class'].count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgaZDfH5-WK9",
        "outputId": "8bc00b06-fb3b-4901-9bb4-b19de310a1c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 88.24629855155945 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# KNN\n",
        "\n",
        "\"\"\"\n",
        "class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
        "\n",
        "n_neighborsint\n",
        "Number of neighbors to use by default for kneighbors queries.\n",
        "weights{‘uniform’, ‘distance’} or callable, default=’uniform’ - weight function used in prediction. Possible values:\n",
        "algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’ - Algorithm used to compute the nearest neighbors:\n",
        "\n",
        "leaf_sizeint, default=30 - Leaf size passed to BallTree or KDTree.\n",
        "\n",
        "pint, default=2 - Power parameter for the Minkowski metric. \n",
        "\n",
        "metricstr or callable, default=’minkowski’ - the distance metric to use for the tree. \n",
        "\n",
        "metric_paramsdict, default=None - Additional keyword arguments for the metric function.\n",
        "\n",
        "n_jobsint, default=None - The number of parallel jobs to run for neighbors search. \n",
        "\"\"\"\n",
        "start_time = time.time()\n",
        "\n",
        "clf_knn1 = KNeighborsClassifier(n_jobs = -1)\n",
        "clf_knn1.fit(X_train, Y_train)\n",
        "predicted_knn1 = clf_knn1.predict(X_test)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cm_OLIUd_aJj",
        "outputId": "6174719a-bd7e-4696-d6a9-86716bea4557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 290.6679856777191 seconds ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "clf_knn2 = KNeighborsClassifier(n_neighbors=3, weights='distance', n_jobs = -1)\n",
        "clf_knn2.fit(X_train, Y_train)\n",
        "predicted_knn2 = clf_knn2.predict(X_test)\n",
        "predicted_knn_train = clf_knn2.predict(X_train)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rm3oxFEXrhwF",
        "outputId": "16d62cdd-8d03-4824-c569-70d0eea70b1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['1', '0', '5', ..., '4', '6', '8'], dtype=object)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oOtnAhDc9yR9",
        "outputId": "d067033d-d5b2-48cf-e290-ce2e04f5e03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       624\n",
            "           1       0.91      1.00      0.95       625\n",
            "           2       0.98      0.95      0.96       642\n",
            "           3       0.96      0.96      0.96       617\n",
            "           4       0.96      0.94      0.95       583\n",
            "           5       0.94      0.95      0.95       501\n",
            "           6       0.97      0.98      0.98       567\n",
            "           7       0.96      0.95      0.96       630\n",
            "           8       0.98      0.89      0.93       613\n",
            "           9       0.92      0.94      0.93       598\n",
            "\n",
            "    accuracy                           0.96      6000\n",
            "   macro avg       0.96      0.96      0.96      6000\n",
            "weighted avg       0.96      0.96      0.96      6000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98       624\n",
            "           1       0.94      0.99      0.96       625\n",
            "           2       0.98      0.94      0.96       642\n",
            "           3       0.96      0.96      0.96       617\n",
            "           4       0.96      0.95      0.96       583\n",
            "           5       0.95      0.95      0.95       501\n",
            "           6       0.98      0.98      0.98       567\n",
            "           7       0.96      0.95      0.95       630\n",
            "           8       0.99      0.91      0.95       613\n",
            "           9       0.91      0.95      0.93       598\n",
            "\n",
            "    accuracy                           0.96      6000\n",
            "   macro avg       0.96      0.96      0.96      6000\n",
            "weighted avg       0.96      0.96      0.96      6000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(metrics.classification_report(Y_test, predicted_knn1))\n",
        "print(metrics.classification_report(Y_test, predicted_knn2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xjF4J-Zr-uCm",
        "outputId": "b9ebb026-690c-4278-87d5-6a09b4f9cf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier predicted\n",
            "Accuracy Tree: 0.8168333333333333\n",
            "Accuracy Tree: 1.0\n"
          ]
        }
      ],
      "source": [
        "#DecisionTreeClassifier\n",
        "\"\"\"\n",
        "class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
        "max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
        "\n",
        "criterion{“gini”, “entropy”}, default=”gini” \n",
        "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
        "\n",
        "splitter{“best”, “random”}, default=”best” \n",
        "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
        "\n",
        "max_depthint, default=None \n",
        "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "min_samples_splitint or float, default=2 \n",
        "The minimum number of samples required to split an internal node.\n",
        "\n",
        "min_samples_leafint or float, default=1 \n",
        "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
        "This may have the effect of smoothing the model, especially in regression.\n",
        "\n",
        "min_weight_fraction_leaffloat, default=0.0\n",
        "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
        "\n",
        "max_featuresint, float or {“auto”, “sqrt”, “log2”}, default=None \n",
        "The number of features to consider when looking for the best split\n",
        "\n",
        "random_stateint, RandomState instance, default=None \n",
        "Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to \"best\". \n",
        "When max_features < n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. \n",
        "That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. \n",
        "\n",
        "max_leaf_nodesint, default=None\n",
        "Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "\n",
        "min_impurity_decreasefloat, default=0.0\n",
        "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "\n",
        "min_impurity_splitfloat, default=0 \n",
        "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
        "\n",
        "class_weightdict, list of dict or “balanced”, default=None \n",
        "Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
        "\n",
        "ccp_alphanon-negative float, default=0.0\n",
        "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. \n",
        "\"\"\"\n",
        "\n",
        "clf_tree = DecisionTreeClassifier()\n",
        "clf_tree.fit(X_train, Y_train)\n",
        "predicted_tree = clf_tree.predict(X_test)\n",
        "predicted_tree_train1 = clf_tree.predict(X_train)\n",
        "\n",
        "\n",
        "print(\"DecisionTreeClassifier predicted\")\n",
        "\n",
        "print(\"Accuracy Tree:\", metrics.accuracy_score(Y_test, predicted_tree))\n",
        "print(\"Accuracy Tree:\", metrics.accuracy_score(Y_train, predicted_tree_train1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXNXgzMQVk3O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YPYla3V4Ob4h",
        "outputId": "51cbeb38-4765-4bfd-c106-5d2c326f010d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier predicted\n",
            "Accuracy Tree: 0.8440714285714286\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn import tree\\nplt.figure(figsize = [30,20])\\ntree.plot_tree(clf_tree2, filled = True, fontsize = 8)\\nplt.show()\\n'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf_tree2 = DecisionTreeClassifier(splitter = \"best\",max_depth = 8, min_samples_split = 5, min_samples_leaf = 5, random_state = 2,\n",
        "                                    class_weight = \"balanced\" )\n",
        "clf_tree2.fit(X_train, Y_train)\n",
        "predicted_tree2 = clf_tree2.predict(X_test)\n",
        "predicted_tree_train = clf_tree2.predict(X_train)\n",
        "print(\"DecisionTreeClassifier predicted\")\n",
        "\n",
        "print(\"Accuracy Tree:\", metrics.accuracy_score(Y_train, predicted_tree_train))\n",
        "\n",
        "\"\"\"\n",
        "from sklearn import tree\n",
        "plt.figure(figsize = [30,20])\n",
        "tree.plot_tree(clf_tree2, filled = True, fontsize = 8)\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZDBTQb1LOg2U",
        "outputId": "5e9815c8-263f-49eb-f3ef-4cfd62b32275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.89       624\n",
            "           1       0.89      0.94      0.92       625\n",
            "           2       0.83      0.74      0.78       642\n",
            "           3       0.77      0.78      0.77       617\n",
            "           4       0.84      0.82      0.83       583\n",
            "           5       0.75      0.75      0.75       501\n",
            "           6       0.84      0.86      0.85       567\n",
            "           7       0.85      0.84      0.85       630\n",
            "           8       0.76      0.73      0.74       613\n",
            "           9       0.73      0.81      0.77       598\n",
            "\n",
            "    accuracy                           0.82      6000\n",
            "   macro avg       0.82      0.82      0.82      6000\n",
            "weighted avg       0.82      0.82      0.82      6000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.92       624\n",
            "           1       0.87      0.91      0.89       625\n",
            "           2       0.81      0.75      0.78       642\n",
            "           3       0.81      0.73      0.76       617\n",
            "           4       0.70      0.78      0.74       583\n",
            "           5       0.71      0.71      0.71       501\n",
            "           6       0.84      0.81      0.83       567\n",
            "           7       0.83      0.81      0.82       630\n",
            "           8       0.76      0.72      0.74       613\n",
            "           9       0.68      0.77      0.72       598\n",
            "\n",
            "    accuracy                           0.79      6000\n",
            "   macro avg       0.79      0.79      0.79      6000\n",
            "weighted avg       0.80      0.79      0.79      6000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(metrics.classification_report(Y_test, predicted_tree))\n",
        "print(metrics.classification_report(Y_test, predicted_tree2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s-EvgltF-xH2"
      },
      "outputs": [],
      "source": [
        "#SVM\n",
        "\"\"\"\n",
        "Cfloat, default=1.0\n",
        "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
        "\n",
        "kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
        "Specifies the kernel type to be used in the algorithm. \n",
        "\n",
        "degreeint, default=3\n",
        "Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
        "\n",
        "gamma{‘scale’, ‘auto’} or float, default=’scale’\n",
        "Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
        "\n",
        "coef0float, default=0.0\n",
        "Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
        "\n",
        "shrinkingbool, default=True\n",
        "Whether to use the shrinking heuristic.\n",
        "\n",
        "probabilitybool, default=False\n",
        "Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation, \n",
        "and predict_proba may be inconsistent with predict.\n",
        "\n",
        "tolfloat, default=1e-3\n",
        "Tolerance for stopping criterion.\n",
        "\n",
        "cache_sizefloat, default=200\n",
        "Specify the size of the kernel cache (in MB).\n",
        "\n",
        "class_weightdict or ‘balanced’, default=None\n",
        "Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. \n",
        "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
        "\n",
        "verbosebool, default=False\n",
        "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.\n",
        "\n",
        "max_iterint, default=-1\n",
        "Hard limit on iterations within solver, or -1 for no limit.\n",
        "\n",
        "decision_function_shape{‘ovo’, ‘ovr’}, default=’ovr’\n",
        "Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers, \n",
        "or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). \n",
        "However, one-vs-one (‘ovo’) is always used as multi-class strategy. The parameter is ignored for binary classification.\n",
        "\n",
        "break_tiesbool, default=False\n",
        "If true, decision_function_shape='ovr', and number of classes > 2, predict will break ties according to the confidence values of decision_function; \n",
        "otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict.\n",
        "\n",
        "New in version 0.22.\n",
        "\n",
        "random_stateint or RandomState instance, default=None\n",
        "Controls the pseudo random number generation for shuffling the data for probability estimates. \n",
        "Ignored when probability is False. Pass an int for reproducible output across multiple function calls. See Glossary.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "clf_svm_1 = svm.SVC()\n",
        "clf_svm_1.fit(X_train, Y_train)\n",
        "\n",
        "predicted_svm_1 = clf_svm_1.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6J9rr4BEbcjI",
        "outputId": "b1989b63-ffe4-4b6c-a074-eb7446dbe5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm predicted\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       624\n",
            "           1       0.97      0.99      0.98       625\n",
            "           2       0.96      0.96      0.96       642\n",
            "           3       0.97      0.95      0.96       617\n",
            "           4       0.94      0.96      0.95       583\n",
            "           5       0.96      0.97      0.96       501\n",
            "           6       0.97      0.98      0.97       567\n",
            "           7       0.98      0.96      0.97       630\n",
            "           8       0.96      0.95      0.96       613\n",
            "           9       0.95      0.95      0.95       598\n",
            "\n",
            "    accuracy                           0.96      6000\n",
            "   macro avg       0.96      0.96      0.96      6000\n",
            "weighted avg       0.96      0.96      0.96      6000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       624\n",
            "           1       0.98      0.99      0.99       625\n",
            "           2       0.97      0.97      0.97       642\n",
            "           3       0.98      0.95      0.97       617\n",
            "           4       0.95      0.97      0.96       583\n",
            "           5       0.97      0.96      0.97       501\n",
            "           6       0.97      0.98      0.97       567\n",
            "           7       0.98      0.96      0.97       630\n",
            "           8       0.96      0.96      0.96       613\n",
            "           9       0.96      0.97      0.97       598\n",
            "\n",
            "    accuracy                           0.97      6000\n",
            "   macro avg       0.97      0.97      0.97      6000\n",
            "weighted avg       0.97      0.97      0.97      6000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf_svm_o1 = svm.SVC(C= 2.75, kernel='rbf', class_weight = 'balanced')\n",
        "clf_svm_o1.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "predicted_svm_o1 = clf_svm_o1.predict(X_test)\n",
        "\n",
        "print(\"svm predicted\")\n",
        "\n",
        "print(metrics.classification_report(Y_test, predicted_svm_1))\n",
        "print(metrics.classification_report(Y_test, predicted_svm_o1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SqbNzNJJ-xQi"
      },
      "outputs": [],
      "source": [
        "#LogReg\n",
        "\"\"\"\n",
        "penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’\n",
        "Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver. \n",
        "If ‘none’ (not supported by the liblinear solver), no regularization is applied.\n",
        "\n",
        "dualbool, default=False\n",
        "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n",
        "\n",
        "tolfloat, default=1e-4\n",
        "Tolerance for stopping criteria.\n",
        "\n",
        "Cfloat, default=1.0\n",
        "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
        "\n",
        "fit_interceptbool, default=True\n",
        "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.\n",
        "\n",
        "intercept_scalingfloat, default=1\n",
        "Useful only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \n",
        "“synthetic” feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic_feature_weight.\n",
        "\n",
        "class_weightdict or ‘balanced’, default=None\n",
        "Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
        "\n",
        "random_stateint, RandomState instance, default=None\n",
        "Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data. See Glossary for details.\n",
        "\n",
        "solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
        "Algorithm to use in the optimization problem.\n",
        "\n",
        "max_iterint, default=100\n",
        "Maximum number of iterations taken for the solvers to converge.\n",
        "\n",
        "multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’\n",
        "If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. \n",
        "‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.\n",
        "\n",
        "verboseint, default=0\n",
        "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.\n",
        "\n",
        "warm_startbool, default=False\n",
        "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver.\n",
        "\n",
        "\"\"\"\n",
        "clf_logreg = LogisticRegression(n_jobs = -1).fit(X_train, Y_train)\n",
        "predicted_logreg = clf_logreg.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j3gSoKUAevSd",
        "outputId": "5da5b350-98a5-4aad-a2fd-2d05c8886a58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        }
      ],
      "source": [
        "clf_logreg2 = LogisticRegression(penalty = 'l1', C=2, class_weight='balanced', solver = 'saga', n_jobs = -1).fit(X_train, Y_train)\n",
        "predicted_logreg2 = clf_logreg2.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6hKFU7O2i9BG",
        "outputId": "6588da75-4db1-4890-fcea-71c1ee598475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94       624\n",
            "           1       0.93      0.97      0.95       625\n",
            "           2       0.90      0.84      0.87       642\n",
            "           3       0.89      0.87      0.88       617\n",
            "           4       0.90      0.91      0.90       583\n",
            "           5       0.81      0.82      0.81       501\n",
            "           6       0.93      0.92      0.92       567\n",
            "           7       0.91      0.89      0.90       630\n",
            "           8       0.87      0.86      0.87       613\n",
            "           9       0.85      0.89      0.87       598\n",
            "\n",
            "    accuracy                           0.89      6000\n",
            "   macro avg       0.89      0.89      0.89      6000\n",
            "weighted avg       0.89      0.89      0.89      6000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       624\n",
            "           1       0.93      0.97      0.95       625\n",
            "           2       0.91      0.85      0.88       642\n",
            "           3       0.91      0.88      0.89       617\n",
            "           4       0.90      0.91      0.91       583\n",
            "           5       0.84      0.84      0.84       501\n",
            "           6       0.93      0.93      0.93       567\n",
            "           7       0.93      0.89      0.91       630\n",
            "           8       0.88      0.88      0.88       613\n",
            "           9       0.85      0.91      0.88       598\n",
            "\n",
            "    accuracy                           0.90      6000\n",
            "   macro avg       0.90      0.90      0.90      6000\n",
            "weighted avg       0.90      0.90      0.90      6000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(metrics.classification_report(Y_test, predicted_logreg))\n",
        "print(metrics.classification_report(Y_test, predicted_logreg2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Task3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNws2At5FQ66C4IiIYdTZmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}